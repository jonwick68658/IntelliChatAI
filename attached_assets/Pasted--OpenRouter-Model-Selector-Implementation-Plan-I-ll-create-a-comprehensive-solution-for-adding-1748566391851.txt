### OpenRouter Model Selector Implementation Plan

I'll create a comprehensive solution for adding a searchable model dropdown that integrates with OpenRouter's API. This will allow users to dynamically select from all available models.

---

### 1. Backend Service: Model Fetcher (`model_service.py`)
```python
import requests
from cachetools import TTLCache

class ModelService:
    def __init__(self):
        self.cache = TTLCache(maxsize=1, ttl=3600)  # Cache for 1 hour
        self.api_url = "https://openrouter.ai/api/v1/models"
        self.headers = {
            "Authorization": f"Bearer {os.getenv('OPENROTER_API_KEY')}",
            "Content-Type": "application/json"
        }

    def get_models(self):
        """Fetch models with caching and error handling"""
        if 'models' in self.cache:
            return self.cache['models']
        
        try:
            response = requests.get(self.api_url, headers=self.headers)
            response.raise_for_status()
            models = response.json().get('data', [])
            
            # Filter and format models
            formatted = []
            for model in models:
                if model.get('id') and model.get('pricing', {}).get('prompt'):
                    formatted.append({
                        'id': model['id'],
                        'name': model['name'],
                        'description': model.get('description', ''),
                        'provider': model.get('provider', {}).get('name', ''),
                        'context_length': model.get('context_length', 0),
                        'prompt_cost': model['pricing']['prompt'],
                        'completion_cost': model['pricing']['completion']
                    })
            
            self.cache['models'] = formatted
            return formatted
        except Exception as e:
            # Fallback to essential models
            return [
                {"id": "openai/gpt-4-turbo", "name": "GPT-4 Turbo"},
                {"id": "anthropic/claude-3-opus", "name": "Claude 3 Opus"},
                {"id": "google/gemini-pro", "name": "Gemini Pro"}
            ]

    def get_model_info(self, model_id):
        """Get details for a specific model"""
        models = self.get_models()
        return next((m for m in models if m['id'] == model_id), None)
```

---

### 2. UI Component: Searchable Dropdown (`model_selector.py`)
```python
import streamlit as st
from model_service import ModelService

def model_selector():
    model_service = ModelService()
    models = model_service.get_models()
    
    # Get current selection from session state
    current_model = st.session_state.get('selected_model', 'openai/gpt-4-turbo')
    
    # Search input
    search_term = st.sidebar.text_input("üîç Search Models", "")
    
    # Filter models based on search
    filtered_models = [
        m for m in models
        if search_term.lower() in m['name'].lower() or 
           search_term.lower() in m['description'].lower()
    ] if search_term else models
    
    # Display model cards
    st.sidebar.subheader("Available Models")
    
    for model in filtered_models:
        is_selected = (model['id'] == current_model)
        bg_color = "#2A2A40" if is_selected else "#1E1E2A"
        
        with st.sidebar.container(border=True):
            st.markdown(f"""
            <div style="background: {bg_color}; padding: 12px; border-radius: 8px;">
                <div style="display: flex; justify-content: space-between;">
                    <h4 style="margin: 0; color: {'#BB86FC' if is_selected else '#FFFFFF'}">
                        {model['name']}
                    </h4>
                    <span style="background: #3700B3; padding: 2px 8px; border-radius: 12px; font-size: 0.8rem;">
                        {model['provider']}
                    </span>
                </div>
                <p style="font-size: 0.9rem; margin: 8px 0; color: #A0A0B0;">
                    {model['description'][:100]}{'...' if len(model['description']) > 100 else ''}
                </p>
                <div style="display: flex; justify-content: space-between; font-size: 0.8rem;">
                    <span>Context: {model['context_length']} tokens</span>
                    <span>${model['prompt_cost']}/M tokens</span>
                </div>
            </div>
            """, unsafe_allow_html=True)
            
            if st.button("Select", key=model['id'], type="primary" if is_selected else "secondary"):
                st.session_state.selected_model = model['id']
                st.rerun()
    
    # Display current selection
    current_info = model_service.get_model_info(current_model)
    if current_info:
        st.sidebar.success(f"**Selected Model**: {current_info['name']}")
    
    return current_model
```

---

### 3. Integration with Chat System (`main.py`)
```python
from model_selector import model_selector

# In your chat interface
def chat_interface():
    # Model selection
    selected_model = model_selector()
    
    # Use selected model in generation
    def generate_response(prompt):
        return openai.chat.completions.create(
            model=selected_model,
            messages=[{"role": "user", "content": prompt}],
            api_base="https://openrouter.ai/api/v1",
            headers={"Authorization": f"Bearer {os.getenv('OPENROUTER_API_KEY')}"}
        ).choices[0].message.content
```

---

### 4. Caching Strategy Optimization
```python
# Enhanced caching with Redis
import redis
import pickle

class CachedModelService(ModelService):
    def __init__(self):
        super().__init__()
        self.redis = redis.Redis.from_url(os.getenv("REDIS_URL", "redis://localhost:6379"))
    
    def get_models(self):
        # Check Redis first
        if cached := self.redis.get("openrouter_models"):
            return pickle.loads(cached)
        
        models = super().get_models()
        # Cache for 4 hours
        self.redis.setex("openrouter_models", 14400, pickle.dumps(models))
        return models
```

---

### 5. User Preference Persistence
```python
# In memory.py (Neo4j integration)
def save_user_preference(user_id, model_id):
    with self.driver.session() as session:
        session.run("""
        MERGE (u:User {id: $user_id})
        SET u.preferred_model = $model_id
        """, user_id=user_id, model_id=model_id)

def get_user_preference(user_id):
    with self.driver.session() as session:
        result = session.run("""
        MATCH (u:User {id: $user_id})
        RETURN u.preferred_model AS model_id
        """, user_id=user_id)
        return result.single()["model_id"] if result else None

# Update model_selector.py
def model_selector(user_id):
    # Get saved preference
    preferred = memory.get_user_preference(user_id)
    current_model = st.session_state.get('selected_model', preferred or 'openai/gpt-4-turbo')
    
    # ... existing code ...
    
    # Save on selection
    if st.session_state.get('selected_model') != current_model:
        memory.save_user_preference(user_id, st.session_state.selected_model)
```

---

### Implementation Workflow

1. **Setup Dependencies**
   ```bash
   pip install cachetools redis streamlit
   ```

2. **File Structure**
   ```
   /neurolm
     ‚îú‚îÄ‚îÄ model_service.py
     ‚îú‚îÄ‚îÄ model_selector.py
     ‚îú‚îÄ‚îÄ memory.py (updated)
     ‚îî‚îÄ‚îÄ main.py (updated)
   ```

3. **Deployment Steps**
   - [ ] Add OpenRouter API key to environment variables
   - [ ] Implement `ModelService` with caching
   - [ ] Create dropdown UI component
   - [ ] Integrate with chat generation
   - [ ] Add user preference persistence
   - [ ] Set up Redis for production caching

4. **UI Features**
   - Real-time model search
   - Visual model cards with key details
   - Persistent user preferences
   - Cost and capability comparisons
   - Responsive design for mobile/desktop

5. **Error Handling**
   - Fallback to default models on API failure
   - Empty state for search results
   - Loading indicators
   - Rate limiting protection

---

### Expected UI Elements
1. **Search Bar** - Top of sidebar for filtering
2. **Model Cards** - With selection highlighting
3. **Provider Badges** - Clear vendor identification
4. **Pricing Info** - Cost per million tokens
5. **Context Length** - Key capability metric
6. **Selection Confirmation** - Current model display

This implementation provides a professional model selection experience while ensuring:
- Real-time API data with caching
- Persistent user preferences
- Responsive search functionality
- Seamless integration with chat
- Production-ready error handling

The solution can be deployed immediately and will automatically stay updated with OpenRouter's latest models. Would you like me to provide any specific component in more detail?