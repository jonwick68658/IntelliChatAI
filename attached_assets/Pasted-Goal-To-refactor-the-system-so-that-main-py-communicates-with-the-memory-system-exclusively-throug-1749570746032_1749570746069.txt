Goal:
To refactor the system so that main.py communicates with the memory system exclusively through the memory_api.py endpoints, and to optimize memory retrieval using Neo4j's native vector search capabilities.

Implementation Plan
This plan is broken into two main phases:

Phase 1: Upgrading the Core Memory System (memory.py) for efficient vector search.
Phase 2: Refactoring the Main Application (main.py) to use the Memory API correctly.
Phase 1: Upgrade memory.py for High-Performance Vector Search
The current retrieve_memories function fetches all of a user's memories and then calculates similarity scores in Python. This is very inefficient. We can leverage Neo4j's built-in vector index for near-instantaneous lookups.

Step 1.1: Create a Vector Index in Neo4j

In memory.py, modify the _create_schema method to create a vector index. This only needs to run once, but having it in the schema function ensures it exists.

File: memory.py
Method: _create_schema()
Action: Add the CREATE VECTOR INDEX query.
# In memory.py, inside the MemorySystem class

    def _create_schema(self):
        """Create the database schema with vector support"""
        with self.driver.session() as session:
            # Create constraints and indexes
            session.run("CREATE CONSTRAINT IF NOT EXISTS FOR (u:User) REQUIRE u.id IS UNIQUE")
            session.run("CREATE CONSTRAINT IF NOT EXISTS FOR (m:MemoryNode) REQUIRE m.id IS UNIQUE")
            # ... (other constraints remain the same)

            # Create the vector index for MemoryNode embeddings
            try:
                session.run("""
                    CREATE VECTOR INDEX memory_embeddings IF NOT EXISTS
                    FOR (m:MemoryNode) ON (m.embedding)
                    OPTIONS { indexConfig: {
                        `vector.dimensions`: 1536,
                        `vector.similarity_function`: 'cosine'
                    }}
                """)
                print("Vector index 'memory_embeddings' is configured.")
            except Exception as e:
                print(f"Note: Vector index may already exist. Error: {e}")
Copy
Note: The dimension 1536 matches OpenAI's text-embedding-3-small model used in utils.py.
Step 1.2: Refactor retrieve_memories to Use the Vector Index

Replace the current manual similarity calculation with a single, highly efficient Cypher query.

File: memory.py
Method: retrieve_memories()
Action: Replace the entire method body with the following logic.
# In memory.py, inside the MemorySystem class

    def retrieve_memories(self, query: str, context: Optional[str] = None, depth: int = 5, user_id: Optional[str] = None) -> List[MemoryNode]:
        """Retrieve relevant memories based on query and context using Neo4j vector search"""
        if not user_id:
            return []

        query_embedding = self._generate_embedding(query)
        if not query_embedding:
            return []

        with self.driver.session() as session:
            result = session.run(
                """
                CALL db.index.vector.queryNodes('memory_embeddings', $depth, $embedding) YIELD node, score
                MATCH (u:User {id: $user_id})-[:HAS_MEMORY]->(node)
                RETURN node, score
                ORDER BY score DESC
                """,
                {
                    "depth": depth,
                    "embedding": query_embedding,
                    "user_id": user_id
                }
            )

            memories = []
            for record in result:
                memory_data = record["node"]
                score = record["score"]

                # Re-create the MemoryNode object from the retrieved data
                memory_node = MemoryNode(
                    content=memory_data.get("content"),
                    confidence=memory_data.get("confidence"),
                    category=memory_data.get("category"),
                    timestamp=memory_data.get("timestamp").to_native()
                )
                memory_node.id = memory_data.get("id")
                # You could potentially store the similarity score on the node if needed
                # memory_node.similarity_score = score
                memories.append(memory_node)

            return memories
Copy
Benefit: This offloads the entire search operation to the database, which is optimized for this task. It eliminates the special-case "name" query and provides a unified, fast, and scalable retrieval mechanism.
Phase 2: Refactor main.py to Use the Memory API
The current chat_with_memory function in main.py bypasses the memory_api.py and interacts directly with the MemorySystem class. This breaks the intended architectural separation. We need to make it call the API endpoints.

Step 2.1: Modify the /api/chat Endpoint

File: main.py
Function: chat_with_memory()
Action: Replace the direct MemorySystem calls with httpx calls to the API endpoints.
# In main.py

# Make sure httpx is imported at the top
import httpx

# ... other code

@app.post("/api/chat", response_model=ChatResponse)
async def chat_with_memory(chat_request: ChatMessage, request: Request):
    """
    Chat with LLM using memory system for context
    """
    try:
        # Extract user_id from session (this part is correct)
        session_id = request.cookies.get("session_id")
        if not session_id or session_id not in user_sessions:
            raise HTTPException(status_code=401, detail="Not authenticated")
        user_id = user_sessions[session_id]['user_id']
        
        # Handle slash commands (this part is correct)
        if chat_request.message.startswith('/'):
            # ... (no changes needed here)

        # --- REFACTORED SECTION ---
        # Instead of calling MemorySystem directly, call the API
        base_url = str(request.base_url)
        api_client = httpx.AsyncClient(base_url=base_url)

        # 1. Retrieve relevant memories via API
        retrieve_payload = {
            "query": chat_request.message,
            "depth": 5,
            "user_id": user_id # This needs to be added to the Pydantic model
        }
        
        # Note: We need to update RetrieveMemoryRequest in memory_api.py to include user_id
        # For now, we will add it to the retrieve_memories call in memory_api.py from the request.
        # Let's adjust the plan slightly.
        # In memory_api.py, the retrieve function needs to get the user_id.
        # Let's assume the API will handle user auth. For now, we'll pass it.

        # Let's update the Pydantic model in memory_api.py first.
        # In memory_api.py -> class RetrieveMemoryRequest: add `user_id: Optional[str] = None`

        # Now, back to main.py:
        response = await api_client.post("/api/retrieve/", json=retrieve_payload)
        response.raise_for_status() # Raise an exception for bad status codes
        relevant_memories_data = response.json()
        
        context = "\n".join([f"- {mem['content']}" for mem in relevant_memories_data])

        # ... (rest of the logic for file context, system prompt, etc., remains largely the same)
        # Just use `context` variable as before.

        # 2. Store user message in memory via API
        memorize_user_payload = {
            "content": f"User said: {chat_request.message}",
            "user_id": user_id
        }
        await api_client.post("/api/memorize/", json=memorize_user_payload)

        # ... (LLM call logic remains the same)
        # from model_services import ModelService # corrected import
        from model_service import ModelService
        model_service = ModelService()
        # ... generate response_text ...
        
        # 3. Store assistant response in memory via API
        memorize_assistant_payload = {
            "content": f"Assistant responded: {response_text}",
            "user_id": user_id
        }
        await api_client.post("/api/memorize/", json=memorize_assistant_payload)

        await api_client.aclose()
        # --- END OF REFACTORED SECTION ---

        # ... (Conversation management logic remains the same)
        
        return ChatResponse(
            response=response_text,
            memory_stored=True,
            context_used=len(relevant_memories_data),
            conversation_id=conversation_id
        )
        
    except httpx.HTTPStatusError as e:
        raise HTTPException(status_code=e.response.status_code, detail=f"Memory API error: {e.response.text}")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Chat error: {str(e)}")
Copy
Step 2.2: Update memory_api.py Models and Endpoints

To support the change in main.py, we need to ensure the API endpoints can receive the user_id.

File: memory_api.py
Action: Update Pydantic models and the retrieve_memories endpoint.
# In memory_api.py

class RetrieveMemoryRequest(BaseModel):
    query: str
    context: Optional[str] = None
    depth: Optional[int] = 5
    user_id: Optional[str] = None # Add this line

# ...

@router.post("/retrieve/", response_model=List[MemoryResponse])
async def retrieve_memories(retrieve_request: RetrieveMemoryRequest):
    """
    Retrieve relevant memories based on query
    """
    try:
        # Pass the user_id from the request to the memory system
        memories = memory_system.retrieve_memories(
            query=retrieve_request.query,
            context=retrieve_request.context or "",
            depth=retrieve_request.depth or 5,
            user_id=retrieve_request.user_id # Add this line
        )
        
        # ... (rest of the function is fine)
Copy
Summary of Plan
Optimize memory.py:

Add a vector index to _create_schema() for fast lookups.
Rewrite retrieve_memories() to use db.index.vector.queryNodes, making it vastly more efficient and scalable.
Refactor main.py:

In the /api/chat endpoint, remove direct calls to MemorySystem.
Use the httpx library to make asynchronous POST requests to the /api/retrieve/ and /api/memorize/ endpoints.
Update memory_api.py:

Add the user_id field to the RetrieveMemoryRequest Pydantic model.
Update the /retrieve/ endpoint to pass the user_id to the memory_system.
This plan aligns the application with a proper microservice-style architecture, where the main app is a "client" of the memory API. It also unlocks the true power of the Neo4j database for memory retrieval.